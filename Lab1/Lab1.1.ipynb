{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (3): Image differences\n",
    "\n",
    "## a (1): Capture images\n",
    "\n",
    "Write a function to capture a sequence of images using Matlab .\n",
    "```\n",
    "Open Matlab and install following packages: here\n",
    "vid = videoinput('dcam', 1, 'F7_YUV422_812x612_mode1');\n",
    "vid.FramesPerTrigger = 1;\n",
    "vid.ReturnedColorspace = 'rgb';\n",
    "preview(vid);\n",
    "```\n",
    "in the command window to create a firewire camera pipeline, then you can capture one frame using:\n",
    "```\n",
    "start(vid);\n",
    "stoppreview(vid);\n",
    "imwrite(getdata(vid), 'image_path_to_save');\n",
    "```\n",
    "where success is a flag that indicates whether the image capture was successful.\n",
    "\n",
    "Write a function to capture a sequence of images using Python:\n",
    "```\n",
    "import cv2\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_FIREWIRE)\n",
    "ret_val, img = cam.read()\n",
    "cv2.imwrite(filename, img)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape hit, closing...\n",
      "captureImage/opencv_frame_154.png written!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#rgb to bgr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A = cv2. imread('img.png',0)\n",
    "\n",
    "# cv2.imshow('windowName', A)\n",
    "# cam = cv2.VideoCapture(0, cv2.CAP_FIREWIRE)\n",
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"test\")\n",
    "path = \"captureImage/\"\n",
    "img_counter = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    cv2.imshow(\"test\", frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "    # ASCII:ESC pressed, exit\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "    # ASCII:SPACE pressed, capture frame\n",
    "        img_name = path + \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        # print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "print(\"{} written!\".format(img_name))\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b (2). Image differences\n",
    "\n",
    "Compute the temporal image derivatives approximated by successive image differences. In the case of a fixed camera and a moving object (e.g. person walking by), try to reliably threshold the temporal derivative to extract only the moving object.\n",
    "\n",
    "##### <span style = \"color:red;\">If you do not use thresholding, what could mistakenly be identified as motion?</span>\n",
    "<span style = \"color:green;\">If we do not use thresholding, there is a lot of noise difference added that is not neccesary motion. Shadows are another thing to consider. Without adding the threshold, there can be an increase in a lot of vectors that are technically background in the final video of the optic flow</span>\n",
    "\n",
    "You need to convert from colorful images to gray scale images before going to next step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "path = \"captureImage/\"\n",
    "path1 = \"replaceImage/\"\n",
    "ImageShape = [480,640]\n",
    "imageWidth = ImageShape[1]\n",
    "imageHeight = ImageShape[0]\n",
    "size = (imageWidth, imageHeight)\n",
    "for i in range(10):\n",
    "    #read 2 images\n",
    "    img_nameNew = path + \"opencv_frame_{}.png\".format(i+1)\n",
    "    img_nameOld = path + \"opencv_frame_{}.png\".format(i)\n",
    "    Inew = cv2. imread(img_nameNew,0)\n",
    "    Iold = cv2. imread(img_nameOld,0) \n",
    "    cv2.imshow(\"test1\", Inew)\n",
    "    cv2.imshow(\"test2\", Iold)\n",
    "    #convert to signed 16\n",
    "    Inew = Inew.astype(np.int16)\n",
    "    Iold = Iold.astype(np.int16)\n",
    "    dIm = np.absolute(np.array(Inew) - np.array(Iold))\n",
    "    # take the abolute difference\n",
    "    dIm = dIm.astype(np.uint8)\n",
    "    while True:\n",
    "        # dIm[dIm>=80] = 255\n",
    "        dIm[dIm<20] = 0\n",
    "        cv2.imshow(\"test\", dIm)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k%256 == 27:\n",
    "            break\n",
    "    cv2.imwrite(path1 + \"opencv_difference_{}.png\".format(i), dIm)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (7): Optical flow\n",
    "\n",
    "Solve for the optic flow. Write a routine that outputs the optic flow vectors given an image sequence. Try for both moving camera and moving objects. Chapter 9 from Stockman and Shapiro has a basic presentation of the required concepts: Motion from 2D Image Sequences. Read the introductory sections cursorily. Pay attention to the definitions in Section 9.3 and then closely look at the optic flow equations in Section 9.3.5\n",
    "\n",
    "Tile the image into blocks of user specified size (try sizes 4x4, 8x8, 16x16)\n",
    "For each block, consider if there is motion (Use the thresholds you derived above). If no temporal change set motion vector to 0.\n",
    "If there is temporal change, compute the spatial derivatives and solve for the x and y optic flow vector.\n",
    "Plot the flow vectors on top of the image using quiver.\n",
    "Submit a movie of your results (images with overlayed flow field). You can use getframe to create a movie. Please make sure the movie file is relatively small.\n",
    "\n",
    "\n",
    "#### Example use of quiver and getframe:\n",
    "```\n",
    "X = (1 : blockSize : imageWidth-1) + blockSize/2;\n",
    "Y = (1 : blockSize : imageHeight-1) + blockSize/2;\n",
    "[X,Y] = meshgrid(X,Y);\n",
    "loop through all images (i){\n",
    "  ...\n",
    "  loop through all blocks (ix,iy){\n",
    "    compute motion vectors ...\n",
    "    U(ix,iy) = xMotion;\n",
    "    V(ix,iy) = yMotion;\n",
    "  }\n",
    "  ...\n",
    "  plot image;\n",
    "  hold on;\n",
    "  quiver(X,Y,U,V);\n",
    "  hold off;\n",
    "  M(i) = getframe;\n",
    "}\n",
    "\n",
    "movie(M);\n",
    "save myMovie M;\n",
    "```\n",
    "If you are getting inconsistent results, you could try testing with artificial motion first. For example:\n",
    "```\n",
    "%im1 = image at time 1\n",
    "%im2 = image at time 2\n",
    "im2 = zeros(size(im1));\n",
    "im2(1:end-1, 1:end-1) = im1(2:end, 2:end);\n",
    "```\n",
    "\n",
    "You may also find it useful to display some of your intermediate images/data.\n",
    "\n",
    "##### <span style = \"color:red;\">Can this optic flow account for any type of motion? If not, give two distinct cases when it will not work well. </span>\n",
    "\n",
    "<span style = \"color:green;\">This optic flow only detects the change in pixels in the 2D acces of the plane of the image. It does nont count for any other motion such as rotation, scale, affine, etc. One case it would not work would be if the object was moving closer towards the camera</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-af0002725a0a>:52: RuntimeWarning: invalid value encountered in true_divide\n",
      "  u, v = -arr / np.linalg.norm(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "blockSize = 32\n",
    "path = \"captureImage/\"\n",
    "ImageShape = [480,640]\n",
    "imageWidth = ImageShape[1]\n",
    "imageHeight = ImageShape[0]\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "result = cv2.VideoWriter('outputOptic32.mp4', fourcc, 20, (imageWidth, imageHeight))\n",
    "# result.release()\n",
    "for index in range(115):\n",
    "    img_nameNew = path + \"opencv_frame_{}.png\".format(index+1)\n",
    "    img_nameOld = path + \"opencv_frame_{}.png\".format(index)\n",
    "    Inew = cv2. imread(img_nameNew,0)\n",
    "    Iold = cv2. imread(img_nameOld,0) \n",
    "    size = (imageWidth, imageHeight)\n",
    "\n",
    "    Inew = Inew.astype(np.float)\n",
    "    Iold = Iold.astype(np.float)\n",
    "    # difference image  \n",
    "    # dIm = np.absolute(np.array(Inew) - np.array(Iold))\n",
    "    dIm = np.array(Inew) - np.array(Iold)\n",
    "    dIm = dIm.astype(np.float)\n",
    "    dIm[np.absolute(dIm)<50 ] = 0\n",
    "    [dUm, dVm] = np.gradient(Iold)\n",
    "    #row, column -> y, x\n",
    "    X = range(1+int(blockSize/2),imageWidth,blockSize)\n",
    "    Y = range(1+int(blockSize/2),imageHeight,blockSize)\n",
    "    U = np.ones((len(Y),len(X)))\n",
    "    V = np.ones((len(Y),len(X)))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Y)):\n",
    "            # get frame\n",
    "            k = j*blockSize\n",
    "            l = i*blockSize\n",
    "            dI = dIm[k:k+blockSize, l:l+blockSize]\n",
    "            dU = dUm[k:k+blockSize, l:l+blockSize]\n",
    "            dV = dVm[k:k+blockSize, l:l+blockSize]\n",
    "            # vector, flatten matrix\n",
    "            dIv = dI.flatten()\n",
    "            dUv = dU.flatten()\n",
    "            dVv = dV.flatten()\n",
    "            dIv = np.array([dIv])\n",
    "            #least squares\n",
    "            M = np.array([dUv, dVv])\n",
    "            dIv = dIv.T\n",
    "            M = M.T\n",
    "            arr = np.linalg.lstsq(M,dIv, rcond=None)[0]\n",
    "            # normalise\n",
    "            u, v = -arr / np.linalg.norm(arr)\n",
    "            #x motion\n",
    "            U[j,i] = u\n",
    "            #y motion\n",
    "            V[j,i] = v\n",
    "\n",
    "    [X,Y] = np.meshgrid(X,Y)\n",
    "    Iold = cv2.imread(path + \"opencv_frame_{}.png\".format(index))\n",
    "    Iold = cv2.cvtColor(Iold, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(frameon = False)\n",
    "    fig.set_size_inches(16, 12)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.], )\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    plt.quiver(X,Y,U,V, minlength=4)\n",
    "    ax.imshow(Iold)\n",
    "    fig.canvas.draw()\n",
    "    # matplotlib fig to numpy\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    image_from_plot = cv2.cvtColor(image_from_plot, cv2.COLOR_BGR2RGB)\n",
    "    image_from_plot = cv2.resize(image_from_plot, (640,480), interpolation = cv2.INTER_AREA)\n",
    "    # write\n",
    "    result.write(image_from_plot)\n",
    "    plt.clf()\n",
    "    plt.close(\"all\")\n",
    "print(\"finished\")\n",
    "result.release()\n",
    "cv2.destroyAllWindows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (Bonus - 2): High dimensional motion\n",
    "\n",
    "Try to extract 3-6 dimensional motion. Parameterize motion using e.g. 4 (x,y,rotation,scale) and 6 (affine) parameters. How many motion parameters can be reliably extracted? How does the block size affect the results? Try both camera motion and object motion sequences. Compare results.\n",
    "\n",
    "Each time you may only be able to show 2dof by using quiver. You can show images two times if you are trying 4 dof, 3 times for 6dof.\n",
    "While visualizing your extracted motion parameters is difficult, you can also validate your solution by warping the image (or just individual blocks) based on the extracted motion parameters, and checking if the result is more similar to the next frame than if you computed the warped image using your solution to Exercise 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.pydown()\n",
    "# high degree traking 4 to 6\n",
    "# pyramid down\n",
    "#warp, Jap matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><h1>Lab 1: Optical flow</h1></center>\n",
    "\n",
    "<center>428/615, Martin Jagersand</center>\n",
    "<br>\n",
    "<font color=\"#FF0000\">\n",
    "Submit code and report electronically through the upload link on the e-Class course webpage.<br>\n",
    "Marks may be given for any of the questions in the assignment, so\n",
    "please answer them all <br>(many only require a one sentence answer).<br>\n",
    "Demo: In your lab session.\n",
    "<!-- Demo: In your lab session during the week following the due date. -->\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<b><font size=\"+1\">Exercise 1 (3):</font></b> <font size=\"+1\">Image differences</font>\n",
    "<blockquote>\n",
    "<p><font size=\"+1\">a (1). Capture images</font>\n",
    "\n",
    "</p><p>Write a function to capture a sequence of images using <code> Matlab </code>.\n",
    "<br>\n",
    "Open Matlab and install following packages: <a href=\"https://www.mathworks.com/hardware-support/dcam.html\">here</a>\n",
    "<br>\n",
    "<code> vid = videoinput('dcam', 1, 'F7_YUV422_812x612_mode1'); </code>\n",
    "<br>\n",
    "<code> vid.FramesPerTrigger = 1; </code>\n",
    "<br>\n",
    "<code> vid.ReturnedColorspace = 'rgb'; </code>\n",
    "<br>\n",
    "<code> preview(vid); </code>\n",
    "<br>\n",
    "in the command window to create a firewire camera pipeline, then you can capture one frame using:\n",
    "<br>\n",
    "<code> start(vid);   </code>\n",
    "<br>\n",
    "<code> stoppreview(vid);  </code>\n",
    "<br>\n",
    "<code> imwrite(getdata(vid), 'image_path_to_save');  </code>\n",
    "<br>\n",
    "where <code> success </code> is a flag that indicates whether the image capture was successful.\n",
    "<br>\n",
    "</p><p>Write a function to capture a sequence of images using <code> Python </code>.\n",
    "<br>\n",
    "<code> import cv2  </code>\n",
    "<br> \n",
    "<code> cam = cv2.VideoCapture(0, cv2.CAP_FIREWIRE)</code>\n",
    "<br> \n",
    "<code> ret_val, img = cam.read() </code>\n",
    "<br> \n",
    "<code> cv2.imwrite(filename, img) </code>\n",
    "<br> \n",
    "<!-- <p>Write a function to capture a sequence of images using <code> xvui2 </code>.\n",
    "(Open Matlab, type <code> xvui2 </code> in the command window, then you can capture one frame using <code> im = MexVision('frame'); </code>.) -->\n",
    " \n",
    "<!-- Function input should\n",
    "include number of frames and time between images (this is exactly what \n",
    "we did in lab 0 exercise 2). -->\n",
    "\n",
    "\n",
    "<!-- Use <code> pause </code> to wait between \n",
    "images. <em>This function is used for the remaining exercises</em> -- to \n",
    "permit testing optic flow and tracking code on a sequence repeatedly. -->\n",
    "\n",
    "</p><p><font size=\"+1\">b (2). Image differences</font>\n",
    "</p><p>Compute the temporal image derivatives approximated by successive image \n",
    "differences. In the case of a fixed camera and a moving object (e.g. \n",
    "person walking by), try to reliably threshold the temporal derivative to \n",
    "extract only the moving object.\n",
    "\n",
    "</p><ul>\n",
    "<li><strong>If you do not use thresholding, what could mistakenly be identified \n",
    "as motion?</strong></li>\n",
    "<li><strong> You need to convert from colorful images to gray scale images before going\n",
    "to next step.</strong></li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<b><font size=\"+1\">Exercise 2 (7):</font></b> <font size=\"+1\">Optical flow</font> \n",
    "<blockquote>\n",
    "<p>Solve for the optic flow. Write a routine that outputs the optic flow \n",
    "vectors given an image sequence. Try for both moving camera and moving \n",
    "objects. Chapter 9 from Stockman and Shapiro has a basic presentation of \n",
    "the required concepts: <a href=\"http://www.cs.ualberta.ca/~jag/courses/HandEye/MotionReadings/ch9.pdf\"> \n",
    "Motion from 2D Image \n",
    "Sequences</a>. Read the introductory sections cursorily. \n",
    "Pay attention to the definitions in Section 9.3 and then closely look at \n",
    "the optic flow equations in Section 9.3.5\n",
    "\n",
    "</p><ul>\n",
    "<li>Tile the image into blocks of user specified size (try sizes 4x4, 8x8, \n",
    "16x16)</li>\n",
    "<li>For each block, consider if there is motion (Use the thresholds you \n",
    "derived above). If no temporal change set motion vector to 0.\n",
    "</li>\n",
    "<li>If there is temporal change, compute the spatial derivatives and solve \n",
    "for the x and y optic flow vector.\n",
    "</li>\n",
    "<li>Plot the flow vectors on top of the image using <code>quiver</code>.\n",
    "</li>\n",
    "<li>Submit a movie of your results (images with overlayed flow \n",
    "field). You can use <code>getframe</code> to create a movie. Please make \n",
    "sure the movie file is relatively small.\n",
    "</li>\n",
    "\n",
    "<pre><code>\n",
    "Example use of quiver and getframe\n",
    "\n",
    "X = (1 : blockSize : imageWidth-1) + blockSize/2;\n",
    "Y = (1 : blockSize : imageHeight-1) + blockSize/2;\n",
    "[X,Y] = meshgrid(X,Y);\n",
    "loop through all images (i){\n",
    "  ...\n",
    "  loop through all blocks (ix,iy){\n",
    "    compute motion vectors ...\n",
    "    U(ix,iy) = xMotion;\n",
    "    V(ix,iy) = yMotion;\n",
    "  }\n",
    "  ...\n",
    "  plot image;\n",
    "  hold on;\n",
    "  quiver(X,Y,U,V);\n",
    "  hold off;\n",
    "  M(i) = getframe;\n",
    "}\n",
    "\n",
    "movie(M);\n",
    "save myMovie M;\n",
    "</code></pre>\n",
    "</ul>\n",
    "\n",
    "If you are getting inconsistent results, you could try testing with \n",
    "artificial motion first. For example:\n",
    "<pre><code>\n",
    "%im1 = image at time 1\n",
    "%im2 = image at time 2\n",
    "im2 = zeros(size(im1));\n",
    "im2(1:end-1, 1:end-1) = im1(2:end, 2:end);\n",
    "</code></pre>\n",
    "You may also find it useful to display some of your intermediate \n",
    "images/data.\n",
    "<br clear=\"all\">\n",
    "<a href=\"optflow.jpg\"><img src=\"optflow.jpg\" width=\"100\"></a>\n",
    "<br clear=\"all\">\n",
    "<ul>\n",
    "<li><strong>Can this optic flow account for any type of motion? If not, give two \n",
    "distinct cases when it will not work well.</strong></li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<b><font size=\"+1\">Exercise 3 (Bonus - 2):</font></b> <font size=\"+1\">High dimensional motion</font>\n",
    "<blockquote>\n",
    "\n",
    "Try to extract 3-6 dimensional motion. Parameterize motion using e.g. 4 (x,y,rotation,scale) and 6 (affine) parameters.\n",
    "How many motion parameters can be reliably extracted? How does the block size affect the results? Try both camera motion and object motion\n",
    "sequences. Compare results.\n",
    "<ul>\n",
    "<li>Each time you may only be able to show 2dof by using quiver. You can show images two\n",
    "times if you are trying 4 dof, 3 times for 6dof.</li>\n",
    "<li>While visualizing your extracted motion parameters is difficult, you can also validate your solution by warping the image (or just individual blocks) based on the extracted motion parameters, and checking if the result is more similar to the next frame than if you computed the warped image using your solution to Exercise 2.</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<h3>Test sequences (Use as desired — It is not mandatory for you to submit results on them)</h3>\n",
    "<ul>\n",
    "<li> How to read and interpret the image sequences. <a href=\"ReadIm.m\">matlab m-\n",
    "file</a>\n",
    "</li><li> A 60 image sequence with a jade tree <a href=\"Flower60im3.mat\">Flower60im3.\n",
    "mat</a>\n",
    "</li><li> A 32 image sequence with articulated arm movement <a href=\"armD32im1.mat\">a\n",
    "rmD32im1.mat</a>. This is the same sequence as used in the <a href=\"armD32im1.mp\n",
    "g\">movie</a> above.\n",
    "</li></ul>\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad17c2651b7428e7e567fe245ea5c5c4158643910a3341d44bd7925570c7d39a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
